{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645be6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections.abc import Iterable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29844aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 10]) torch.Size([30, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedLinearDecoder(nn.Module):\n",
    "    \"\"\"Linear decoder for scVI with hard mask on its regression weights.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        mask: torch.Tensor,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # 1) keep mask as a buffer (not a parameter)\n",
    "        #    shape must be [out_features, in_features] == [n_output, n_input]\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "        # 2) build your normal 1-layer FCLayers that outputs 2*n_output units\n",
    "        self.linear = nn.Linear(n_input, n_output)\n",
    "\n",
    "        # 4) zero out masked positions at init\n",
    "        with torch.no_grad():\n",
    "            print(self.linear.weight.shape, self.mask.shape)\n",
    "            self.linear.weight.mul_(self.mask)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # 5) re-apply mask on every forward under no_grad\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.mul_(self.mask)\n",
    "\n",
    "        # 6) proceed as before\n",
    "        return self.linear(z)\n",
    "\n",
    "mask = torch.tensor(np.random.choice([0,1], size=(30,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_hidden: int = 128,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.shared_decoder = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.gene_velocity_decoder = nn.Sequential(\n",
    "                nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "\n",
    "        self.gp_velocity_decoder = nn.Sequential(\n",
    "                nn.Linear(n_hidden, n_input)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Parameters for latent distribution\n",
    "        h = self.shared_decoder(z)\n",
    "        velocity = self.gene_velocity_decoder(h)\n",
    "        velocity_gp = self.gp_velocity_decoder(h)\n",
    "\n",
    "        return velocity, velocity_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        # shared encoder MLP\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # project to mean and log-variance\n",
    "        self.mean_layer   = nn.Linear(n_hidden, n_latent)\n",
    "        self.logvar_layer = nn.Linear(n_hidden, n_latent)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h      = self.encoder(x)\n",
    "        mean   = self.mean_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        z      = self.reparametrize(mean, logvar)\n",
    "        return z, mean, logvar\n",
    "\n",
    "    @staticmethod\n",
    "    def reparametrize(mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e96ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    n_input=200,\n",
    "    n_latent=10,\n",
    "    n_hidden=128\n",
    ")\n",
    "\n",
    "x = torch.randn((20, 200))\n",
    "z, mean, logvar = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8479dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 10]) torch.Size([30, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoder = MaskedLinearDecoder(\n",
    "    10,\n",
    "    30,\n",
    "    mask\n",
    ")\n",
    "\n",
    "z = torch.randn((100,10))\n",
    "out = decoder(z)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac16e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "velo_decoder = VelocityDecoder(\n",
    "    n_input=10,\n",
    "    n_output=30,\n",
    "    n_hidden=128\n",
    ")\n",
    "\n",
    "velo, velo_gp = velo_decoder(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
